{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10550636,"sourceType":"datasetVersion","datasetId":6412205},{"sourceId":10894059,"sourceType":"datasetVersion","datasetId":6757972}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install efficientnet_pytorch\n!pip install torcheval","metadata":{"_uuid":"8c2f271d-ad1f-487a-8918-4b212cf436bb","_cell_guid":"c19e3090-a927-45c4-a866-02b4f5141b92","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:00:49.476117Z","iopub.execute_input":"2025-03-02T02:00:49.476498Z","iopub.status.idle":"2025-03-02T02:01:00.080504Z","shell.execute_reply.started":"2025-03-02T02:00:49.476474Z","shell.execute_reply":"2025-03-02T02:01:00.079611Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse\nimport math\nimport os\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom typing import Optional\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom efficientnet_pytorch import EfficientNet\nfrom PIL import Image\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn import init\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torcheval.metrics.functional import multiclass_f1_score\nfrom torchsummary import summary\nfrom torchvision import transforms\nfrom torchvision.transforms import v2\nfrom tqdm import tqdm\nimport plotly.express as px\nimport json\nfrom copy import deepcopy","metadata":{"_uuid":"cd75b461-5fac-419c-a4cb-afd8f228bb7c","_cell_guid":"30b3f8b0-7971-495d-af4f-7582df3e7d2f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:02.421883Z","iopub.execute_input":"2025-03-02T02:01:02.422204Z","iopub.status.idle":"2025-03-02T02:01:14.111241Z","shell.execute_reply.started":"2025-03-02T02:01:02.422159Z","shell.execute_reply":"2025-03-02T02:01:14.110530Z"},"id":"7y6Lt3lbC1EZ","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xceptionnet_pretrained_weights = \"/kaggle/input/wai-challenge-data/xceptionnet_FF_weights.pth\"\ntrain_csv = \"/kaggle/input/ai-vs-human-generated-dataset/train.csv\"\ndata = \"/kaggle/input/ai-vs-human-generated-dataset\"\ntrain_folder = \"/kaggle/input/ai-vs-human-generated-dataset/train_data\"\ntest_folder = \"/kaggle/input/ai-vs-human-generated-dataset/test_data_v2\"\ntest_csv = \"/kaggle/input/ai-vs-human-generated-dataset/test.csv\"\nefficientnetB4_pretrained_weights = \"/kaggle/input/wai-challenge-data/EfficientNetB4_FFPP_bestval-93aaad84946829e793d1a67ed7e0309b535e2f2395acb4f8d16b92c0616ba8d7.pth\"\ntest_image_sizes_df = pd.read_csv(\"/kaggle/input/wai-challenge-data/test_image_sizes\")\ntrain_image_sizes_df = pd.read_csv(\"/kaggle/input/wai-challenge-data/train_image_sizes\")\nxceptionnet_best_params = \"/kaggle/input/wai-challenge-data/xceptionnet_best_weights.pth\"\nefficinetnetB4_best_params = \"/kaggle/input/wai-challenge-data/efficientnetB4_best_weights.pth\"","metadata":{"_uuid":"6d840bd1-70e1-465e-805f-8cb74fe4d10d","_cell_guid":"2afe0912-13d5-4327-8732-a0be3e36252d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:27:39.083787Z","iopub.execute_input":"2025-03-02T05:27:39.084067Z","iopub.status.idle":"2025-03-02T05:27:39.176291Z","shell.execute_reply.started":"2025-03-02T05:27:39.084045Z","shell.execute_reply":"2025-03-02T05:27:39.175503Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nPorted to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n\n@author: tstandley\nAdapted by cadene\n\nCreates an Xception Model as defined in:\n\nFrancois Chollet\nXception: Deep Learning with Depthwise Separable Convolutions\nhttps://arxiv.org/pdf/1610.02357.pdf\n\nThis weights ported from the Keras implementation. Achieves the following performance on the validation set:\n\nLoss:0.9173 Prec@1:78.892 Prec@5:94.292\n\nREMEMBER to set your image size to 3x299x299 for both test and validation\n\nnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                  std=[0.5, 0.5, 0.5])\n\nThe resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n\"\"\"\n\npretrained_settings = {\n    \"xception\": {\n        \"imagenet\": {\n            \"url\": xceptionnet_pretrained_weights,\n            \"input_space\": \"RGB\",\n            \"input_size\": [3, 299, 299],\n            \"input_range\": [0, 1],\n            \"mean\": [0.5, 0.5, 0.5],\n            \"std\": [0.5, 0.5, 0.5],\n            \"num_classes\": 1000,\n            \"scale\": 0.8975,  # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n        }\n    }\n}\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=1,\n        stride=1,\n        padding=0,\n        dilation=1,\n        bias=False,\n    ):\n        super(SeparableConv2d, self).__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels,\n            in_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups=in_channels,\n            bias=bias,\n        )\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(\n        self,\n        in_filters,\n        out_filters,\n        reps,\n        strides=1,\n        start_with_relu=True,\n        grow_first=True,\n    ):\n        super(Block, self).__init__()\n\n        if out_filters != in_filters or strides != 1:\n            self.skip = nn.Conv2d(\n                in_filters, out_filters, 1, stride=strides, bias=False\n            )\n            self.skipbn = nn.BatchNorm2d(out_filters)\n        else:\n            self.skip = None\n\n        self.relu = nn.ReLU(inplace=True)\n        rep = []\n\n        filters = in_filters\n        if grow_first:\n            rep.append(self.relu)\n            rep.append(\n                SeparableConv2d(\n                    in_filters, out_filters, 3, stride=1, padding=1, bias=False\n                )\n            )\n            rep.append(nn.BatchNorm2d(out_filters))\n            filters = out_filters\n\n        for i in range(reps - 1):\n            rep.append(self.relu)\n            rep.append(\n                SeparableConv2d(filters, filters, 3, stride=1, padding=1, bias=False)\n            )\n            rep.append(nn.BatchNorm2d(filters))\n\n        if not grow_first:\n            rep.append(self.relu)\n            rep.append(\n                SeparableConv2d(\n                    in_filters, out_filters, 3, stride=1, padding=1, bias=False\n                )\n            )\n            rep.append(nn.BatchNorm2d(out_filters))\n\n        if not start_with_relu:\n            rep = rep[1:]\n        else:\n            rep[0] = nn.ReLU(inplace=False)\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3, strides, 1))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x += skip\n        return x\n\n\nclass Xception(nn.Module):\n    \"\"\"\n    Xception optimized for the ImageNet dataset, as specified in\n    https://arxiv.org/pdf/1610.02357.pdf\n    \"\"\"\n\n    def __init__(self, num_classes=1000):\n        \"\"\"Constructor\n        Args:\n            num_classes: number of classes\n        \"\"\"\n        super(Xception, self).__init__()\n        self.num_classes = num_classes\n\n        self.conv1 = nn.Conv2d(3, 32, 3, 2, 0, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, 64, 3, bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        # do relu here\n\n        self.dropout = nn.Dropout(p=0.2)\n\n        self.block1 = Block(64, 128, 2, 2, start_with_relu=False, grow_first=True)\n        self.block2 = Block(128, 256, 2, 2, start_with_relu=True, grow_first=True)\n        self.block3 = Block(256, 728, 2, 2, start_with_relu=True, grow_first=True)\n\n        self.block4 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block5 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block6 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block7 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n\n        self.block8 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block9 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block10 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block11 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n\n        self.block12 = Block(728, 1024, 2, 2, start_with_relu=True, grow_first=False)\n\n        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1)\n        self.bn3 = nn.BatchNorm2d(1536)\n\n        # do relu here\n        self.conv4 = SeparableConv2d(1536, 2048, 3, 1, 1)\n        self.bn4 = nn.BatchNorm2d(2048)\n\n        self.fc = nn.Linear(2048, num_classes)\n\n        # #------- init weights --------\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d):\n        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         m.weight.data.fill_(1)\n        #         m.bias.data.zero_()\n        # #-----------------------------\n\n    def features(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        return x\n\n    def logits(self, features):\n        x = self.relu(features)\n\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, input):\n        x = self.features(input)\n        x = self.logits(x)\n        return x\n\n\ndef xception(num_classes=1000, pretrained=\"imagenet\"):\n    model = Xception(num_classes=num_classes)\n    if pretrained:\n        settings = pretrained_settings[\"xception\"][pretrained]\n        assert (\n            num_classes == settings[\"num_classes\"]\n        ), \"num_classes should be {}, but is {}\".format(\n            settings[\"num_classes\"], num_classes\n        )\n\n        model = Xception(num_classes=num_classes)\n        model.load_state_dict(torch.load(settings[\"url\"], weights_only=True))\n\n        model.input_space = settings[\"input_space\"]\n        model.input_size = settings[\"input_size\"]\n        model.input_range = settings[\"input_range\"]\n        model.mean = settings[\"mean\"]\n        model.std = settings[\"std\"]\n\n    # TODO: ugly\n    model.last_linear = model.fc\n    del model.fc\n    return model","metadata":{"_uuid":"c0af48fb-1cc4-4fd5-81bf-6a1665c8d68c","_cell_guid":"ae7188b4-61f4-4b10-a603-9d8656b41303","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:16.030409Z","iopub.execute_input":"2025-03-02T02:01:16.030759Z","iopub.status.idle":"2025-03-02T02:01:16.053283Z","shell.execute_reply.started":"2025-03-02T02:01:16.030730Z","shell.execute_reply":"2025-03-02T02:01:16.052404Z"},"id":"MiJ9u-kGC1EW","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nVideo Face Manipulation Detection Through Ensemble of CNNs\n\nImage and Sound Processing Lab - Politecnico di Milano\n\nNicolò Bonettini\nEdoardo Daniele Cannas\nSara Mandelli\nLuca Bondi\nPaolo Bestagini\n\"\"\"\n\n\"\"\"\nFeature Extractor\n\"\"\"\n\n\nclass FeatureExtractor(nn.Module):\n    \"\"\"\n    Abstract class to be extended when supporting features extraction.\n    It also provides standard normalized and parameters\n    \"\"\"\n\n    def features(self, x: torch.Tensor) -> torch.Tensor:\n        raise NotImplementedError\n\n    def get_trainable_parameters(self):\n        return self.parameters()\n\n    @staticmethod\n    def get_normalizer():\n        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n\n\"\"\"\nEfficientNet\n\"\"\"\n\n\nclass EfficientNetGen(FeatureExtractor):\n    def __init__(self, model: str):\n        super(EfficientNetGen, self).__init__()\n\n        self.efficientnet = EfficientNet.from_pretrained(model)\n        self.classifier = nn.Linear(self.efficientnet._conv_head.out_channels, 1)\n        del self.efficientnet._fc\n\n    def features(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.efficientnet.extract_features(x)\n        x = self.efficientnet._avg_pooling(x)\n        x = x.flatten(start_dim=1)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.efficientnet._dropout(x)\n        x = self.classifier(x)\n        return x\n\n\nclass EfficientNetB4(EfficientNetGen):\n    def __init__(self):\n        super(EfficientNetB4, self).__init__(model='efficientnet-b4')","metadata":{"_uuid":"b68e32dd-66c6-4abb-85ad-9500a6151af2","_cell_guid":"eeff8e05-9489-4da2-9f28-fa359c5af621","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:20.677678Z","iopub.execute_input":"2025-03-02T02:01:20.678018Z","iopub.status.idle":"2025-03-02T02:01:20.685456Z","shell.execute_reply.started":"2025-03-02T02:01:20.677992Z","shell.execute_reply":"2025-03-02T02:01:20.684636Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"learning_rate = 0.001\nweight_decay = 0.0001\nbatch_size = 128\nseed = 42\nepochs = 5\n\nconfigs = {\n    \"xceptionnet\": {\n        \"network\": lambda: XceptionNetwork(),\n        \"best_params\": xceptionnet_best_params,\n        \"learning_rate\": learning_rate,\n        \"weight_decay\": weight_decay,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs,\n        \"img_size\": 299,\n        \"logs\": {\n            \"train_losses\": [],\n            \"valid_losses\": [],\n            \"train_f1_scores\": [],\n            \"valid_f1_scores\": []\n        },\n    },\n    \"efficientnetB4\": {\n        \"network\": lambda: EfficientNetB4Network(),\n        \"best_params\": efficinetnetB4_best_params,\n        \"learning_rate\": learning_rate,\n        \"weight_decay\": weight_decay,\n        \"batch_size\": batch_size,\n        \"epochs\": epochs,\n        \"img_size\": 224,\n        \"logs\": {\n            \"train_losses\": [],\n            \"valid_losses\": [],\n            \"train_f1_scores\": [],\n            \"valid_f1_scores\": []\n        }\n    },\n}","metadata":{"_uuid":"8c125674-66a4-4fcf-8592-e2a0e2e397dc","_cell_guid":"ab288a6d-0074-40f5-8d44-4fc95163bb75","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:28:18.624823Z","iopub.execute_input":"2025-03-02T05:28:18.625177Z","iopub.status.idle":"2025-03-02T05:28:18.630994Z","shell.execute_reply.started":"2025-03-02T05:28:18.625144Z","shell.execute_reply":"2025-03-02T05:28:18.629947Z"},"id":"LFkv8_7NC1EZ","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from copy import deepcopy\n\ndef save_config(configs, model_name):\n    \"\"\"Saves model configuration and training logs to a JSON file with a timestamp.\"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    filename = f\"{model_name}_config_{timestamp}.json\"\n\n    # Remove lambda functions (JSON doesn't support them)\n    config_to_save = deepcopy(configs)\n    config_to_save.pop(\"network\", None)\n\n    # Save configuration to a JSON file\n    with open(filename, \"w\") as f:\n        json.dump(config_to_save, f, indent=4)\n\n    print(f\"Configuration saved to {filename}\")","metadata":{"_uuid":"da654d3c-e310-4d16-aa51-0bc84a606a7f","_cell_guid":"0f694df8-3bfa-4c5a-87e9-fb9e84905fc9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:27.650871Z","iopub.execute_input":"2025-03-02T02:01:27.651162Z","iopub.status.idle":"2025-03-02T02:01:27.656421Z","shell.execute_reply.started":"2025-03-02T02:01:27.651140Z","shell.execute_reply":"2025-03-02T02:01:27.655414Z"},"_kg_hide-input":true,"id":"K6Rx-bDdC1Eb","outputId":"e46c182e-be0f-4343-b1ff-010deaebefbf","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n\n    def _fit(self):\n        print(f\"INITIALIZING TRAINING ON CUDA GPU\")\n        start_time = datetime.now()\n        print(f\"Start Time: {start_time}\")\n\n        valid_loss_min = np.inf\n        \n        for epoch in range(1,  configs[model_name][\"epochs\"]+1):\n            print(f\"{'='*50}\")\n            print(f\"EPOCH {epoch} - TRAINING...\")\n        \n            epoch_loss = 0.0\n            epoch_f1_score = 0.0\n        \n            model.train()\n            for data, target in tqdm(train_dataloader):\n                data = data.to(device, dtype=torch.float32)\n                target = target.to(device, dtype=torch.long)\n        \n                optimizer.zero_grad()\n        \n                output = model(data)\n                loss = criterion(output, target)\n        \n                loss.backward()\n        \n                #f_score = f1_score(target.cpu(), output.cpu().argmax(dim=1))\n                f_score = multiclass_f1_score(output, target, num_classes=2)\n                epoch_loss += loss.item()\n                epoch_f1_score += f_score.item()\n        \n                optimizer.step()\n        \n            train_loss, train_f1_score = epoch_loss / len(train_dataloader), epoch_f1_score / len(train_dataloader)\n            print(f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, F1-SCORE: {train_f1_score}\\n\")\n            configs[model_name][\"logs\"][\"train_losses\"].append(train_loss)\n            configs[model_name][\"logs\"][\"train_f1_scores\"].append(train_f1_score)\n        \n            print(f\"EPOCH {epoch} - VALIDATING...\")\n        \n            valid_loss = 0.0\n            valid_f1_score = 0.0\n        \n            model.eval()\n        \n            for data, target in val_dataloader:\n                data = data.to(device, dtype=torch.float32)\n                target = target.to(device, dtype=torch.long)\n        \n                with torch.no_grad():\n                    output = model(data)\n                    loss = criterion(output, target)\n                    #f_score = f1_score(target.cpu(), output.cpu().argmax(dim=1))\n                    f_score = multiclass_f1_score(output, target, num_classes=2)\n                    valid_loss += loss.item()\n                    valid_f1_score += f_score.item()\n        \n            val_loss, val_f1_socre = valid_loss / len(val_dataloader), valid_f1_score / len(val_dataloader)\n            print(f\"\\t[VALID] LOSS: {val_loss}, F1-SCORE: {val_f1_socre}\\n\")\n            configs[model_name][\"logs\"][\"valid_losses\"].append(val_loss)\n            configs[model_name][\"logs\"][\"valid_f1_scores\"].append(val_f1_socre)\n        \n            if val_loss <= valid_loss_min and epoch != 1:\n                print(\"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(valid_loss_min, val_loss))\n                torch.save(model.state_dict(),f\"{model_name}_best_weights.pth\")\n                valid_loss_min = val_loss\n    \n        save_config(configs[model_name], model_name)\n        print(f\"Execution time: {datetime.now() - start_time}\")\n        \n        torch.save(model.state_dict,f'{model_name}_weights_{datetime.now().strftime(\"%Y%m%d-%H%M\")}.pth')","metadata":{"_uuid":"0ad43cf7-3c85-49ad-b520-b72293c566b3","_cell_guid":"0ac3d2ff-1997-416f-ae7d-444ddc9ecb78","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:31.578212Z","iopub.execute_input":"2025-03-02T02:01:31.578571Z","iopub.status.idle":"2025-03-02T02:01:31.588996Z","shell.execute_reply.started":"2025-03-02T02:01:31.578540Z","shell.execute_reply":"2025-03-02T02:01:31.587990Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define EfficientNetB4 Network\nclass EfficientNetB4Network(Network):\n    def __init__(self):\n        super(EfficientNetB4Network, self).__init__()\n        num_classes=2\n        self.model = self._init_efficientnetB4(num_classes)\n\n    def _init_efficientnetB4(self, num_classes):\n        \"\"\"Initializes EfficientNetB4 with a modified classifier.\"\"\"\n        model = EfficientNetB4()\n        model.load_state_dict(\n            torch.load(\n                efficientnetB4_pretrained_weights,\n                weights_only=True\n            )\n        )\n\n        self._freeze_params(model)\n\n        # Modify classifier\n        in_features = model.classifier.in_features\n        model.classifier = nn.Linear(in_features, num_classes)\n\n        # Unfreeze specific layers\n        self._unfreeze_params(model.efficientnet._blocks[-1]._project_conv)\n        self._unfreeze_params(model.efficientnet._conv_head)\n        self._unfreeze_params(model.classifier)\n\n        return model\n\n    def forward(self, x):\n        return self.model(x)\n\n    def _freeze_params(self, module):\n        for param in module.parameters():\n            param.requires_grad = False\n\n    def _unfreeze_params(self, module):\n        for param in module.parameters():\n            param.requires_grad = True","metadata":{"_uuid":"2efbbb90-892c-4734-a70b-ea8371a17db1","_cell_guid":"ecfa95ae-49bc-4f60-adce-fca3cd5e3b24","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:34.116365Z","iopub.execute_input":"2025-03-02T02:01:34.116650Z","iopub.status.idle":"2025-03-02T02:01:34.123060Z","shell.execute_reply.started":"2025-03-02T02:01:34.116628Z","shell.execute_reply":"2025-03-02T02:01:34.122202Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Xception Network\nclass XceptionNetwork(Network):\n    def __init__(self):\n        super(XceptionNetwork, self).__init__()\n        num_classes=2\n        self.model = self._init_xceptionnet(num_classes)\n\n    def _init_xceptionnet(self, num_classes):\n        \"\"\"Initializes Xception model with a modified classifier.\"\"\"\n        model = xception()\n        self._freeze_params(model)\n\n        # Modify classifier\n        in_features = model.last_linear.in_features\n        model.last_linear = nn.Sequential(\n            nn.Dropout(p=0.5), nn.Linear(in_features, num_classes)\n        )\n\n        # Unfreeze specific layers\n        self._unfreeze_params(model.last_linear)\n        self._unfreeze_params(model.conv4)\n\n        return model\n\n    def forward(self, x):\n        return self.model(x)\n\n    def _freeze_params(self, module):\n        for param in module.parameters():\n            param.requires_grad = False\n\n    def _unfreeze_params(self, module):\n        for param in module.parameters():\n            param.requires_grad = True","metadata":{"_uuid":"85346a6e-c01b-4f09-b917-c15b798bf1f1","_cell_guid":"20247412-0e9d-4a28-a3a7-343bf1e77f3c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:36.751360Z","iopub.execute_input":"2025-03-02T02:01:36.751666Z","iopub.status.idle":"2025-03-02T02:01:36.757979Z","shell.execute_reply.started":"2025-03-02T02:01:36.751641Z","shell.execute_reply":"2025-03-02T02:01:36.757029Z"},"id":"oq8-bANZC1Eb","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_img_sizes = []\ntest_img_sizes = []\n\nfor file_path in os.listdir(train_folder):\n    im = Image.open(os.path.join(train_folder,file_path))\n    width, height = im.size\n    train_img_sizes.append((file_path, width, height))\n\nfor file_path in os.listdir(test_folder):\n    im = Image.open(os.path.join(test_folder,file_path))\n    width, height = im.size\n    test_img_sizes.append((file_path, width, height))\n\ntrain_img_sizes_df = pd.DataFrame(data=train_img_sizes, columns=[\"filename\", \"width\", \"height\"])\ntrain_img_sizes_df.to_csv(\"train_image_sizes\", index=False)\n\ntest_img_sizes_df = pd.DataFrame(data=test_img_sizes, columns=[\"filename\", \"width\", \"height\"])\ntest_img_sizes_df.to_csv(\"test_image_sizes\", index=False)","metadata":{"_uuid":"b9a0dee8-d99f-419b-bef5-b8543d164d21","_cell_guid":"c9812028-d503-4526-b6d4-123b2e7d19db","trusted":true,"execution":{"iopub.status.busy":"2025-02-28T06:01:07.472890Z","iopub.execute_input":"2025-02-28T06:01:07.473251Z","iopub.status.idle":"2025-02-28T06:09:44.047349Z","shell.execute_reply.started":"2025-02-28T06:01:07.473226Z","shell.execute_reply":"2025-02-28T06:09:44.045743Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute frequency count of (width, height)\nfreq_df = train_image_sizes_df.value_counts(subset=['width', 'height']).reset_index()\nfreq_df.columns = ['width', 'height', 'count']\n\n# Apply a transformation to make smaller counts more visible\nfreq_df['scaled_size'] = np.sqrt(freq_df['count']) * 3  # Scale up small values\n\n# Create scatter plot with size representing frequency\nfig = px.scatter(\n    freq_df, \n    x=\"width\", \n    y=\"height\", \n    size=\"count\",  # Adjusted size for visibility\n    title=\"Frequency Distribution of Image Sizes\",\n    opacity=0.7\n)\n\nfig.update_layout(\n    xaxis_title=\"Width (pixels)\",\n    yaxis_title=\"Height (pixels)\",\n    showlegend=False,\n    hovermode=\"closest\",\n    width=800,\n    height=600,\n    margin=dict(l=50, r=50, b=50, t=50, pad=4)\n)\n\nfig.show()","metadata":{"_uuid":"a23c2c52-d489-4653-a1cb-cc8c0ae4bee8","_cell_guid":"d7539e6a-54e9-4d29-8eb0-1d52e6f30823","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-01T02:51:40.119908Z","iopub.execute_input":"2025-03-01T02:51:40.120283Z","iopub.status.idle":"2025-03-01T02:51:40.190842Z","shell.execute_reply.started":"2025-03-01T02:51:40.120253Z","shell.execute_reply":"2025-03-01T02:51:40.189716Z"},"jupyter":{"source_hidden":true,"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_image_sizes_df.describe()","metadata":{"_uuid":"7cfaf26d-6bb8-4224-a815-596eb78657b1","_cell_guid":"cf9b2d40-5e1b-47da-970a-775f70bbf930","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-01T03:07:19.566422Z","iopub.execute_input":"2025-03-01T03:07:19.566768Z","iopub.status.idle":"2025-03-01T03:07:19.590332Z","shell.execute_reply.started":"2025-03-01T03:07:19.566740Z","shell.execute_reply":"2025-03-01T03:07:19.589293Z"},"jupyter":{"source_hidden":true,"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute frequency count of (width, height)\nfreq_df = test_image_sizes_df.value_counts(subset=['width', 'height']).reset_index()\nfreq_df.columns = ['width', 'height', 'count']\n\n# Apply a transformation to make smaller counts more visible\nfreq_df['scaled_size'] = np.sqrt(freq_df['count']) * 3  # Scale up small values\n\n# Create scatter plot with size representing frequency\nfig = px.scatter(\n    freq_df, \n    x=\"width\", \n    y=\"height\", \n    size=\"count\",  # Adjusted size for visibility\n    title=\"Frequency Distribution of Image Sizes\",\n    opacity=0.7\n)\n\nfig.update_layout(\n    xaxis_title=\"Width (pixels)\",\n    yaxis_title=\"Height (pixels)\",\n    showlegend=False,\n    hovermode=\"closest\",\n    width=800,\n    height=600,\n    margin=dict(l=50, r=50, b=50, t=50, pad=4)\n)\n\nfig.show()","metadata":{"_uuid":"bcd00b85-52d6-4b03-ac79-7c294ef56cc3","_cell_guid":"1cbd572b-0550-4555-bdba-78d48e295e24","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-01T03:08:31.229075Z","iopub.execute_input":"2025-03-01T03:08:31.229466Z","iopub.status.idle":"2025-03-01T03:08:31.293698Z","shell.execute_reply.started":"2025-03-01T03:08:31.229436Z","shell.execute_reply":"2025-03-01T03:08:31.292634Z"},"jupyter":{"source_hidden":true,"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_image_sizes_df.describe()","metadata":{"_uuid":"c8ac0a69-7b38-4fe5-bcf4-83b2c7e26aac","_cell_guid":"a7a792e0-deb6-4e68-a779-3e73ffde9e20","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-01T03:08:07.914480Z","iopub.execute_input":"2025-03-01T03:08:07.914865Z","iopub.status.idle":"2025-03-01T03:08:07.930790Z","shell.execute_reply.started":"2025-03-01T03:08:07.914832Z","shell.execute_reply":"2025-03-01T03:08:07.929848Z"},"jupyter":{"source_hidden":true,"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_data_transforms(model_name, is_train=True):\n    \"\"\"Returns the appropriate transformation pipeline based on the model.\"\"\"\n    \n    input_size = configs[model_name][\"img_size\"]\n    if is_train:\n        transform = v2.Compose(\n            [\n                v2.ToTensor(),\n                v2.Resize(333),\n                v2.CenterCrop(input_size),\n                v2.RandomHorizontalFlip(p=0.5),\n                v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n                v2.GaussianNoise(),\n            ]\n        )\n    else:\n        transform = v2.Compose(\n            [\n                v2.ToTensor(),\n                v2.Resize(512),\n                v2.CenterCrop(input_size)\n            ]\n        )  \n    return transform","metadata":{"_uuid":"b8d69891-9281-4fce-8e44-e71c54884f8e","_cell_guid":"18289c89-4e00-4035-ae7b-cac5710915c2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:39.386575Z","iopub.execute_input":"2025-03-02T02:01:39.386857Z","iopub.status.idle":"2025-03-02T02:01:39.392122Z","shell.execute_reply.started":"2025-03-02T02:01:39.386836Z","shell.execute_reply":"2025-03-02T02:01:39.391182Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, csv_file, image_column, label_column=None, transform=None):\n        self.data = pd.read_csv(csv_file, delimiter=\",\")\n        self.transform = transform\n        self.filenames = self.data[image_column].tolist()\n        self.labels = self.data[label_column].tolist() if label_column else None\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, index: int):\n        img_path = os.path.join(data, self.filenames[index])\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n\n        if self.labels is not None:\n            return image, self.labels[index]\n        return image, self.filenames[index]","metadata":{"_uuid":"215e7d38-74b9-43c5-a2ce-688d0e9dd315","_cell_guid":"8bbf91aa-94db-42ef-a041-12fd3206542d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:41.299993Z","iopub.execute_input":"2025-03-02T02:01:41.300327Z","iopub.status.idle":"2025-03-02T02:01:41.306153Z","shell.execute_reply.started":"2025-03-02T02:01:41.300302Z","shell.execute_reply":"2025-03-02T02:01:41.305071Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"xceptionnet\"\n\ntrain_transform = get_data_transforms(model_name=model_name, is_train=True)\ndataset = ImageDataset(\n    csv_file=train_csv,\n    image_column=\"file_name\",\n    label_column=\"label\",\n    transform=train_transform\n)\n\ntest_transform = get_data_transforms(model_name=model_name, is_train=False)\ntest_dataset = ImageDataset(\n    csv_file=test_csv,\n    image_column=\"id\",\n    transform=test_transform\n)\n\nvalidation_split = 0.2\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\nnp.random.seed(seed)\ntrain_indices, val_indices = indices[split:], indices[:split]\n\ntrain_dataset = Subset(dataset, train_indices)\nval_dataset = Subset(dataset, val_indices)\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=configs[model_name][\"batch_size\"], shuffle=True\n)\nval_dataloader = DataLoader(\n    val_dataset, \n    batch_size=configs[model_name][\"batch_size\"],\n    shuffle=True\n)\n\ntest_dataloader = DataLoader(\n    test_dataset, \n    batch_size=configs[model_name][\"batch_size\"]\n)\n\ntorch.manual_seed(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = configs[model_name][\"network\"]()\nmodel.to(device)\n\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=configs[model_name][\"learning_rate\"],\n    weight_decay=configs[model_name][\"weight_decay\"]\n)\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"_uuid":"2b2315af-0f60-470e-a30a-fe9c3d0c909f","_cell_guid":"c071efca-76d8-47e3-ae5d-6af28b8ba9b8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:43.966512Z","iopub.execute_input":"2025-03-02T02:01:43.966825Z","iopub.status.idle":"2025-03-02T02:01:46.293103Z","shell.execute_reply.started":"2025-03-02T02:01:43.966801Z","shell.execute_reply":"2025-03-02T02:01:46.292423Z"},"id":"qZr1W8ThC1Ea","outputId":"60b4109c-6d21-4c94-d6c5-10da710486a7","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model._fit()","metadata":{"_uuid":"10954908-d84d-4a38-afb6-831d3cee42c4","_cell_guid":"5af5e7bd-c8bf-4c1d-a6f1-1ab04abb6644","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T02:01:49.976571Z","iopub.execute_input":"2025-03-02T02:01:49.976858Z","iopub.status.idle":"2025-03-02T05:21:28.644708Z","shell.execute_reply.started":"2025-03-02T02:01:49.976838Z","shell.execute_reply":"2025-03-02T05:21:28.643718Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(configs[model_name]['best_params'], map_location=torch.device('cpu')))\nmodel.eval()\npredictions = {}\n\nwith torch.no_grad():\n    for images, filenames in tqdm(test_dataloader):\n        images = images.to(device, dtype=torch.float32)\n\n        # Forward pass\n        outputs = model(images)\n        probs = F.softmax(outputs, dim=1)  # Get probabilities\n        preds = torch.argmax(probs, dim=1)  # Get class predictions\n\n        # Store results\n        for filename, pred in zip(filenames, preds.cpu().numpy()):\n            predictions[filename] = pred","metadata":{"_uuid":"f903b7c2-839f-4832-82bb-32e92fabdb9e","_cell_guid":"11ca630a-c9d6-4db4-b1e3-2219d0251ff9","trusted":true,"collapsed":false,"jupyter":{"source_hidden":true,"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_base_models_predictions(models, dataloader):\n    meta_inputs = []\n    targets = []\n        \n    with torch.no_grad():\n        for images, labels in tqdm(dataloader):\n            images = images.to(device, dtype=torch.float32)\n\n            # Initialize predictions container for this batch\n            batch_meta_inputs = []\n\n            # Get predictions from each base model\n            for base_model in base_models:\n                base_model.to(device)\n                base_model.eval()\n                outputs = base_model(images)\n                probs = F.softmax(outputs, dim=1)\n                preds, _ = torch.max(probs, dim=1)\n                batch_meta_inputs.append(preds.cpu().numpy())\n\n            # Concatenate model predictions\n            combined_features = np.stack(batch_meta_inputs, axis=1)\n            meta_inputs.extend(combined_features)\n            targets.extend(labels)\n\n    return np.array(meta_inputs), np.array(targets)","metadata":{"_uuid":"9c8cf615-95ce-428e-84ed-07acb7ea2eb3","_cell_guid":"b1ec0556-d442-4e75-8bca-fbbd8e23734d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:29:49.078908Z","iopub.execute_input":"2025-03-02T05:29:49.079261Z","iopub.status.idle":"2025-03-02T05:29:49.085023Z","shell.execute_reply.started":"2025-03-02T05:29:49.079231Z","shell.execute_reply":"2025-03-02T05:29:49.084219Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_models = []\nfor model_name in configs.keys():\n        base_model = configs[model_name][\"network\"]()\n        print(model_name)\n        best_params = configs[model_name][\"best_params\"]\n        base_model.load_state_dict(torch.load(best_params, map_location=torch.device('cpu'), weights_only= True))\n        base_models.append(base_model)\n\nmeta_train_inputs, meta_targets = get_base_models_predictions(base_models, val_dataloader)\nmeta_test_inputs, filenames = get_base_models_predictions(base_models, test_dataloader)\n\nnp.save(\"meta_train.npy\",meta_train_inputs)\nnp.save(\"meta_label.npy\", meta_targets)\nnp.save(\"meta_test.npy\", meta_test_inputs)","metadata":{"_uuid":"cc60dd6d-12a6-4545-a293-d75d8fc6e30e","_cell_guid":"935985f9-4ef7-4d02-b54e-9a746d269926","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:29:53.208892Z","iopub.execute_input":"2025-03-02T05:29:53.209221Z","iopub.status.idle":"2025-03-02T05:45:53.385265Z","shell.execute_reply.started":"2025-03-02T05:29:53.209160Z","shell.execute_reply":"2025-03-02T05:45:53.384208Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf = LogisticRegression(random_state=0).fit(meta_train_inputs, meta_targets)","metadata":{"_uuid":"f997cac5-7109-4fe6-90a6-4b2b945ef87f","_cell_guid":"3484a0b6-7834-4162-9645-d4a894a71565","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:53:50.094985Z","iopub.execute_input":"2025-03-02T05:53:50.095367Z","iopub.status.idle":"2025-03-02T05:53:50.142700Z","shell.execute_reply.started":"2025-03-02T05:53:50.095335Z","shell.execute_reply":"2025-03-02T05:53:50.141919Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = clf.predict(meta_test_inputs)","metadata":{"_uuid":"1150b7eb-693f-4d33-b331-36e006b3fc0c","_cell_guid":"b1be2d69-e4a9-4179-8361-83da0b2c9cbf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T05:55:13.494751Z","iopub.execute_input":"2025-03-02T05:55:13.495086Z","iopub.status.idle":"2025-03-02T05:55:13.499886Z","shell.execute_reply.started":"2025-03-02T05:55:13.495061Z","shell.execute_reply":"2025-03-02T05:55:13.498958Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.DataFrame({\"id\": filenames, \"label\": preds})\ndf.to_csv(\"preds.csv\")","metadata":{"_uuid":"167bb004-5ce3-4554-9fad-0bd3c741c500","_cell_guid":"716e6d59-1a1d-4c7a-aec3-608f5fe72057","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-02T06:01:41.780973Z","iopub.execute_input":"2025-03-02T06:01:41.781363Z","iopub.status.idle":"2025-03-02T06:01:41.803202Z","shell.execute_reply.started":"2025-03-02T06:01:41.781331Z","shell.execute_reply":"2025-03-02T06:01:41.802369Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = logs[\"train_losses\"]\nvalid_losses = logs[\"valid_losses\"]\ntrain_f1_scores = logs[\"train_f1_scores\"]\nvalid_f1_scores  = logs[\"valid_f1_scores\"]\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(\n    np.arange(1, len(train_losses) + 1),\n    train_losses,\n    label=\"train loss\",\n    marker=\"o\",\n)\nplt.plot(\n    np.arange(1, len(valid_losses) + 1),\n    valid_losses,\n    label=\"validation loss\",\n    marker=\"o\",\n)\nplt.title(\"loss: train vs validation\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(\n    np.arange(1, len(train_f1_scores) + 1),\n    train_f1_scores,\n    label=\"train F1-score\",\n    marker=\"o\",\n)\nplt.plot(\n    np.arange(1, len(valid_f1_scores) + 1),\n    valid_f1_scores,\n    label=\"validation F1-score\",\n    marker=\"o\",\n)\nplt.title(\"Loss: Train vs Validation\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.grid()\n\nplt.subplot(1, 2, 2)\nplt.plot(\n    np.arange(1, epochs + 1), train_f1_scores, label=\"Train  F1-score\", marker=\"o\"\n)\nplt.plot(\n    np.arange(1, epochs + 1),\n    valid_f1_scores,\n    label=\"Validation F1-score\",\n    marker=\"o\",\n)\nplt.title(\"F1-score: Train vs Validation\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"F1-score\")\nplt.legend()\nplt.grid()\n\nplt.tight_layout()\nplt.savefig(\"train_val_metrics.png\")\nplt.show()","metadata":{"_uuid":"2c793532-59c8-4333-81f7-930d3bf16ccc","_cell_guid":"8e84e922-3903-481e-9f88-54b23a99d6f5","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-02-27T00:08:40.225Z"},"id":"y52Voa3KC1Ed","outputId":"2b1bac11-b732-4633-ef6b-a863a0c76824","jupyter":{"source_hidden":true,"outputs_hidden":false}},"outputs":[],"execution_count":null}]}